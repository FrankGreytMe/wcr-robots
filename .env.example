# API keys for different providers
OPENAI_API_KEY=
AZURE_OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GOOGLE_API_KEY=
GROQ_API_KEY=
OPENROUTER_API_KEY=
USE_AWS_BEDROCK=false

#Vertex AI
GOOGLE_APPLICATION_CREDENTIALS=

# Amazon Bedrock Knowledge Base ID
AWS_KB_ID="<knowledge-base-id>"

# Use a fake model for testing
USE_FAKE_MODEL=false

# Set a default model
DEFAULT_MODEL=

# If MODEL is set to "openai-compatible", set the following
# This is just a flexible solution. If you need multiple model options, you still need to add it to models.py
COMPATIBLE_MODEL=
COMPATIBLE_API_KEY=
COMPATIBLE_BASE_URL=

# Web server configuration
#HOST=localhost
#default ye tha 0.0.0.0
#PORT=8080



# Authentication secret, HTTP bearer token header is required if set
AUTH_SECRET=
#Ma9m1zmnlDcHY0XBgozXR5g4bP16mcYRnOgHXjzsLMw=

# Langsmith configuration
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=
# LANGSMITH_PROJECT=default
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# # Application mode. If the value is "dev", it will enable uvicorn reload
# MODE=

# Database type.
# If the value is "postgres", then it will require Postgresql related environment variables.
# If the value is "sqlite", then you can configure optional file path via SQLITE_DB_PATH
DATABASE_TYPE=postgres

# If DATABASE_TYPE=sqlite (Optional)
SQLITE_DB_PATH=




# Server bind (so other containers can reach it)
HOST=0.0.0.0
#HOST=localhost
PORT=8080
# Postgres (use the Compose service name!)
POSTGRES_HOST=postgres
#POSTGRES_HOST=localhost
POSTGRES_PORT=5432
#POSTGRES_PORT=5431
POSTGRES_USER=postgres
POSTGRES_PASSWORD=1538879
#POSTGRES_PASSWORD=password
POSTGRES_DB=chat_history
#POSTGRES_DB=accounting



# OpenWeatherMap API key
OPENWEATHERMAP_API_KEY=

# Add for running ollama
# OLLAMA_MODEL=llama3.2
# Note: set OLLAMA_BASE_URL if running service in docker and ollama on bare metal
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# Add for running Azure OpenAI
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_VERSION=2024-10-21
# AZURE_OPENAI_DEPLOYMENT_MAP={"gpt-4o": "gpt-4o-deployment", "gpt-4o-mini": "gpt-4o-mini-deployment"}

# Agent URL: used in Streamlit app - if not set, defaults to http://{HOST}:{PORT}
# AGENT_URL=http://0.0.0.0:8080

# LANGFUSE Configuration
LANGFUSE_TRACING=true
LANGFUSE_SECRET_KEY=sk-lf-98f7540b-0a39-42cd-bd5d-26410d35cfb1
LANGFUSE_PUBLIC_KEY=pk-lf-ccda7b94-eb98-4212-9db9-1acaf399b5d3
LANGFUSE_HOST=https://langfuse.wcr.is

# (Optional) OTEL to Langfuse cloud:
# OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf
# OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://https://langfuse.wcr.is/api/public/otel/v1/traces
# OTEL_EXPORTER_OTLP_HEADERS=Authorization=Basic <base64(PK:SK)>
# OTEL_EXPORTER_OTLP_TRACES_TIMEOUT=30000

# # Vector Client
# VEC_CLIENT=postgres_timescale_vec_client

